{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa6cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1239f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.5, 0.5, 0.5), \n",
    "            (0.5, 0.5, 0.5)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "train_set = CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_set = CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9163e7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bad0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=1, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer6 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_layer7 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer9 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.conv_layer10 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layer1 = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layer2 = nn.Linear(512, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = self.conv_layer5(x)\n",
    "        x = self.conv_layer6(x)\n",
    "        x = self.conv_layer7(x)\n",
    "        x = self.conv_layer8(x)\n",
    "        x = self.conv_layer9(x)\n",
    "        x = self.conv_layer10(x)\n",
    "        x = self.conv_layer11(x)\n",
    "        x = self.conv_layer12(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layer1(x)\n",
    "        out = self.fc_layer2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f6f6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv_layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer5): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer7): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer8): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer9): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer10): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer11): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv_layer12): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc_layer1): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_layer2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel(10)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c86c35",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a30826",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71635a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0 \n",
    "    running_correct = 0 \n",
    "    total = 0 \n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * running_correct/ total\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b8161c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "max_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12e0ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.0634, Accuracy: 19.17%, Test Loss: 1.8044, Test Accuracy: 28.59%\n",
      "Epoch [2/50], Loss: 1.6518, Accuracy: 36.54%, Test Loss: 1.5042, Test Accuracy: 42.66%\n",
      "Epoch [3/50], Loss: 1.3731, Accuracy: 48.57%, Test Loss: 1.2939, Test Accuracy: 52.52%\n",
      "Epoch [4/50], Loss: 1.1673, Accuracy: 57.18%, Test Loss: 1.1543, Test Accuracy: 58.55%\n",
      "Epoch [5/50], Loss: 1.0074, Accuracy: 63.53%, Test Loss: 0.9964, Test Accuracy: 64.37%\n",
      "Epoch [6/50], Loss: 0.8805, Accuracy: 68.10%, Test Loss: 0.9403, Test Accuracy: 66.46%\n",
      "Epoch [7/50], Loss: 0.7816, Accuracy: 71.89%, Test Loss: 0.8636, Test Accuracy: 69.63%\n",
      "Epoch [8/50], Loss: 0.6918, Accuracy: 75.36%, Test Loss: 0.7809, Test Accuracy: 72.79%\n",
      "Epoch [9/50], Loss: 0.6208, Accuracy: 77.98%, Test Loss: 0.7987, Test Accuracy: 72.37%\n",
      "Epoch [10/50], Loss: 0.5507, Accuracy: 80.53%, Test Loss: 0.7466, Test Accuracy: 75.19%\n",
      "Epoch [11/50], Loss: 0.4918, Accuracy: 82.70%, Test Loss: 0.7731, Test Accuracy: 74.51%\n",
      "Epoch [12/50], Loss: 0.4244, Accuracy: 85.09%, Test Loss: 0.7576, Test Accuracy: 75.30%\n",
      "Epoch [13/50], Loss: 0.3914, Accuracy: 86.16%, Test Loss: 0.7801, Test Accuracy: 76.60%\n",
      "Epoch [14/50], Loss: 0.3395, Accuracy: 88.15%, Test Loss: 0.8419, Test Accuracy: 75.29%\n",
      "Epoch [15/50], Loss: 0.3084, Accuracy: 89.10%, Test Loss: 0.7765, Test Accuracy: 77.74%\n",
      "Epoch [16/50], Loss: 0.2682, Accuracy: 90.43%, Test Loss: 0.8598, Test Accuracy: 77.05%\n",
      "Epoch [17/50], Loss: 0.2395, Accuracy: 91.44%, Test Loss: 0.8953, Test Accuracy: 76.31%\n",
      "Epoch [18/50], Loss: 0.2213, Accuracy: 92.26%, Test Loss: 0.9321, Test Accuracy: 76.14%\n",
      "Epoch [19/50], Loss: 0.1869, Accuracy: 93.45%, Test Loss: 0.9578, Test Accuracy: 77.01%\n",
      "Epoch [20/50], Loss: 0.1641, Accuracy: 94.17%, Test Loss: 0.9698, Test Accuracy: 77.05%\n",
      "Epoch [21/50], Loss: 0.1527, Accuracy: 94.70%, Test Loss: 1.0902, Test Accuracy: 76.46%\n",
      "Epoch [22/50], Loss: 0.1485, Accuracy: 94.71%, Test Loss: 1.0264, Test Accuracy: 77.29%\n",
      "Epoch [23/50], Loss: 0.1336, Accuracy: 95.49%, Test Loss: 1.0818, Test Accuracy: 76.54%\n",
      "Epoch [24/50], Loss: 0.1217, Accuracy: 95.72%, Test Loss: 1.0534, Test Accuracy: 76.78%\n",
      "Epoch [25/50], Loss: 0.1157, Accuracy: 95.97%, Test Loss: 1.1076, Test Accuracy: 76.97%\n",
      "Epoch [26/50], Loss: 0.1000, Accuracy: 96.56%, Test Loss: 1.1736, Test Accuracy: 77.77%\n",
      "Epoch [27/50], Loss: 0.1038, Accuracy: 96.46%, Test Loss: 1.3668, Test Accuracy: 75.38%\n",
      "Epoch [28/50], Loss: 0.0939, Accuracy: 96.77%, Test Loss: 1.3115, Test Accuracy: 76.37%\n",
      "Epoch [29/50], Loss: 0.0962, Accuracy: 96.78%, Test Loss: 1.1906, Test Accuracy: 77.10%\n",
      "Epoch [30/50], Loss: 0.0828, Accuracy: 97.19%, Test Loss: 1.2594, Test Accuracy: 76.26%\n",
      "Epoch [31/50], Loss: 0.0897, Accuracy: 96.96%, Test Loss: 1.3521, Test Accuracy: 76.69%\n",
      "Epoch [32/50], Loss: 0.0752, Accuracy: 97.51%, Test Loss: 1.3060, Test Accuracy: 77.03%\n",
      "Epoch [33/50], Loss: 0.0755, Accuracy: 97.45%, Test Loss: 1.2612, Test Accuracy: 77.37%\n",
      "Epoch [34/50], Loss: 0.0859, Accuracy: 97.06%, Test Loss: 1.3458, Test Accuracy: 77.36%\n",
      "Epoch [35/50], Loss: 0.0746, Accuracy: 97.55%, Test Loss: 1.1709, Test Accuracy: 77.67%\n",
      "Epoch [36/50], Loss: 0.0682, Accuracy: 97.79%, Test Loss: 1.3163, Test Accuracy: 77.26%\n",
      "Epoch [37/50], Loss: 0.0723, Accuracy: 97.55%, Test Loss: 1.2708, Test Accuracy: 78.56%\n",
      "Epoch [38/50], Loss: 0.0650, Accuracy: 97.85%, Test Loss: 1.4831, Test Accuracy: 76.63%\n",
      "Epoch [39/50], Loss: 0.0727, Accuracy: 97.56%, Test Loss: 1.3503, Test Accuracy: 77.03%\n",
      "Epoch [40/50], Loss: 0.0686, Accuracy: 97.73%, Test Loss: 1.4934, Test Accuracy: 77.92%\n",
      "Epoch [41/50], Loss: 0.0673, Accuracy: 97.82%, Test Loss: 1.2920, Test Accuracy: 76.54%\n",
      "Epoch [42/50], Loss: 0.0647, Accuracy: 97.85%, Test Loss: 1.3711, Test Accuracy: 77.55%\n",
      "Epoch [43/50], Loss: 0.0739, Accuracy: 97.52%, Test Loss: 1.3640, Test Accuracy: 77.50%\n",
      "Epoch [44/50], Loss: 0.0545, Accuracy: 98.19%, Test Loss: 1.3987, Test Accuracy: 77.80%\n",
      "Epoch [45/50], Loss: 0.0683, Accuracy: 97.68%, Test Loss: 1.4550, Test Accuracy: 76.48%\n",
      "Epoch [46/50], Loss: 0.0622, Accuracy: 97.94%, Test Loss: 1.4844, Test Accuracy: 77.15%\n",
      "Epoch [47/50], Loss: 0.0654, Accuracy: 97.87%, Test Loss: 1.2963, Test Accuracy: 77.76%\n",
      "Epoch [48/50], Loss: 0.0567, Accuracy: 98.15%, Test Loss: 1.4083, Test Accuracy: 77.32%\n",
      "Epoch [49/50], Loss: 0.0566, Accuracy: 98.13%, Test Loss: 1.4605, Test Accuracy: 77.24%\n",
      "Epoch [50/50], Loss: 0.0655, Accuracy: 97.86%, Test Loss: 1.4743, Test Accuracy: 77.08%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "    running_correct = 0 \n",
    "    total = 0 \n",
    "\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_accuracy = 100 * running_correct / total \n",
    "    epoch_loss = running_loss / (i + 1)\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, testloader, criterion)\n",
    "    print(f\"Epoch [{epoch + 1}/{max_epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b15a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
